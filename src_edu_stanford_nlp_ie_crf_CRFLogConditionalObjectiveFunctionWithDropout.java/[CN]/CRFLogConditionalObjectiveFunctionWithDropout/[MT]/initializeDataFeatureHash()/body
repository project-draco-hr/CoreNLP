{
  int macroActiveFeatureTotalCount=0;
  int macroCondensedTotalCount=0;
  int macroDocPosCount=0;
  log.info("initializing data feature hash, sup-data size: " + data.length + ", unsup data size: "+ (totalData.length - data.length));
  dataFeatureHash=new ArrayList<>(totalData.length);
  condensedMap=new ArrayList<>(totalData.length);
  dataFeatureHashByDoc=new int[totalData.length][];
  for (int m=0; m < totalData.length; m++) {
    Map<Integer,Integer> occurPos=new HashMap<>();
    int[][][] aDoc=totalData[m];
    List<Set<Integer>> aList=new ArrayList<>(aDoc.length);
    Set<Integer> setOfFeatures=new HashSet<>();
    for (int i=0; i < aDoc.length; i++) {
      Set<Integer> aSet=new HashSet<>();
      int[][] dataI=aDoc[i];
      for (int j=0; j < dataI.length; j++) {
        int[] dataJ=dataI[j];
        for (        int item : dataJ) {
          if (j == 0) {
            if (occurPos.containsKey(item))             occurPos.put(item,-1);
 else             occurPos.put(item,i);
          }
          aSet.add(item);
        }
      }
      aList.add(aSet);
      setOfFeatures.addAll(aSet);
    }
    macroDocPosCount+=aDoc.length;
    macroActiveFeatureTotalCount+=setOfFeatures.size();
    if (CONDENSE) {
      if (DEBUG3)       log.info("Before condense, activeFeatures = " + setOfFeatures.size());
      Map<Integer,List<Integer>> condensedFeaturesMap=new HashMap<>();
      int[] representFeatures=new int[aDoc.length];
      Arrays.fill(representFeatures,-1);
      for (      Map.Entry<Integer,Integer> entry : occurPos.entrySet()) {
        int key=entry.getKey();
        int pos=entry.getValue();
        if (pos != -1) {
          if (representFeatures[pos] == -1) {
            representFeatures[pos]=key;
            condensedFeaturesMap.put(key,new ArrayList<>());
          }
 else {
            int rep=representFeatures[pos];
            condensedFeaturesMap.get(rep).add(key);
            aList.get(pos).remove(key);
            setOfFeatures.remove(key);
          }
        }
      }
      int condensedCount=0;
      for (Iterator<Map.Entry<Integer,List<Integer>>> it=condensedFeaturesMap.entrySet().iterator(); it.hasNext(); ) {
        Map.Entry<Integer,List<Integer>> entry=it.next();
        if (entry.getValue().size() == 0) {
          it.remove();
        }
 else {
          if (DEBUG3) {
            condensedCount+=entry.getValue().size();
            for (            int cond : entry.getValue())             log.info("condense " + cond + " to "+ entry.getKey());
          }
        }
      }
      if (DEBUG3)       log.info("After condense, activeFeatures = " + setOfFeatures.size() + ", condensedCount = "+ condensedCount);
      macroCondensedTotalCount+=setOfFeatures.size();
      condensedMap.add(condensedFeaturesMap);
    }
    dataFeatureHash.add(aList);
    int[] arrOfIndex=new int[setOfFeatures.size()];
    int pos2=0;
    for (    Integer ind : setOfFeatures)     arrOfIndex[pos2++]=ind;
    dataFeatureHashByDoc[m]=arrOfIndex;
  }
  log.info("Avg. active features per position: " + (macroActiveFeatureTotalCount / (macroDocPosCount + 0.0)));
  log.info("Avg. condensed features per position: " + (macroCondensedTotalCount / (macroDocPosCount + 0.0)));
  log.info("initializing data feature hash done!");
}
