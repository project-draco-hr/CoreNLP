{
  TokenizerFactory<Word> tokenizerFactory=WhitespaceTokenizer.factory(true);
  Set<String> set=new HashSet<String>();
  set.add("\n");
  WordToSentenceProcessor wordToSent=new WordToSentenceProcessor("",new HashSet<String>(),set);
  System.err.println("Parsing file: " + testFile);
  Document doc=null;
  TreeTransformer d=new Debinarizer(op.forceCNF);
  try {
    doc=new BasicDocument(tokenizerFactory).init(new InputStreamReader(new FileInputStream(testFile),op.langpack().getEncoding()));
  }
 catch (  Exception e) {
    e.printStackTrace();
  }
  if (doc != null) {
    String[] goalStrings=makeGoalStrings();
    List<ArrayList<HasWord>> sentList=wordToSent.processDocument(doc);
    int t=0;
    for (    ArrayList<HasWord> s : sentList) {
      System.err.println("Parsing [len. " + s.size() + "]: "+ Sentence.listToString(s));
      LexicalizedParserQuery pq=lp.parserQuery();
      try {
        pq.parsePCFG(s);
        for (int i=0; i < Subcategory.SUBCATEGORIES.size(); i++) {
          try {
            double score=pq.getPCFGScore(goalStrings[i]);
            System.out.println("score for " + goalStrings[i] + " is: "+ score);
            Tree binaryTree=pq.getBestPCFGParse(false);
            if (binaryTree != null) {
              Tree tree=d.transformTree(binaryTree);
              tree.pennPrint();
            }
          }
 catch (          Throwable e) {
            System.out.println("can't parse to " + goalStrings[i]);
          }
        }
      }
 catch (      UnsupportedOperationException uoe) {
        System.err.println("Whoops, sentence too long.");
      }
    }
  }
}
