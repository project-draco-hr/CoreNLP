{
  System.err.println("Options supplied by this file:");
  System.err.println("  -model <name>: When training, the name of the model to save.  Otherwise, the name of the model to load.");
  System.err.println("  -parser <name>: When training, the LexicalizedParser to use as the base model.");
  System.err.println("  -cachedTrees <name>: The name of the file containing a treebank with cached parses.  See CacheParseHypotheses.java");
  System.err.println("  -treebank <name> [filter]: A treebank to use instead of cachedTrees.  Trees will be reparsed.  Slow.");
  System.err.println("  -testTreebank <name> [filter]: A treebank for testing the model.");
  System.err.println("  -train: Run training over the treebank, testing on the testTreebank.");
  System.err.println("  -continueTraining <name>: The name of a file to continue training.");
  System.err.println("  -nofilter: Rules for the parser will not be filtered based on the training treebank.");
  System.err.println("  -runGradientCheck: Run a gradient check.");
  System.err.println("  -resultsRecord: A file for recording info on intermediate results");
  System.err.println();
  System.err.println("Options overlapping the parser:");
  System.err.println("  -trainingThreads <int>: How many threads to use when training.");
  System.err.println("  -dvKBest <int>: How many hypotheses to use from the underlying parser.");
  System.err.println("  -dvIterations <int>: When training, how many times to go through the train set.");
  System.err.println("  -regCost <double>: How large of a cost to put on regularization.");
  System.err.println("  -dvBatchSize <int>: How many trees to use in each batch of the training.");
  System.err.println("  -qnIterationsPerBatch <int>: How many steps to take per batch.");
  System.err.println("  -qnEstimates <int>: Parameter for qn optimization.");
  System.err.println("  -qnTolerance <double>: Tolerance for early exit when optimizing a batch.");
  System.err.println("  -debugOutputSeconds <int>: How frequently to score a model when training and write out intermediate models.");
  System.err.println("  -maxTrainTimeSeconds <int>: How long to train before terminating.");
  System.err.println("  -dvSeed <long>: A starting point for the random number generator.  Setting this should lead to repeatable results, even taking into account randomness.  Otherwise, a new random seed will be picked.");
  System.err.println("  -wordVectorFile <name>: A filename to load word vectors from.");
  System.err.println("  -numHid: The size of the matrices.  In most circumstances, should be set to the size of the word vectors.");
  System.err.println("  -learningRate: The rate of optimization when training");
  System.err.println("  -deltaMargin: How much we punish trees for being incorrect when training");
  System.err.println("  -(no)unknownNumberVector: Whether or not to use a word vector for unknown numbers");
  System.err.println("  -(no)unknownDashedWordVectors: Whether or not to split unknown dashed words");
  System.err.println("  -(no)unknownCapsVector: Whether or not to use a word vector for unknown words with capitals");
  System.err.println("  -dvSimplifiedModel: Use a greatly dumbed down DVModel");
  System.err.println("  -scalingForInit: How much to scale matrices when creating a new DVModel");
  System.err.println("  -lpWeight: A weight to give the original LexicalizedParser when testing (0.2 seems to work well)");
  System.err.println("  -unkWord: The vector representing unknown word in the word vectors file");
  System.err.println("  -transformMatrixType: A couple different methods for initializing transform matrices");
}
