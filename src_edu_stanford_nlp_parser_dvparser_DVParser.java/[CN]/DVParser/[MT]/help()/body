{
  log.info("Options supplied by this file:");
  log.info("  -model <name>: When training, the name of the model to save.  Otherwise, the name of the model to load.");
  log.info("  -parser <name>: When training, the LexicalizedParser to use as the base model.");
  log.info("  -cachedTrees <name>: The name of the file containing a treebank with cached parses.  See CacheParseHypotheses.java");
  log.info("  -treebank <name> [filter]: A treebank to use instead of cachedTrees.  Trees will be reparsed.  Slow.");
  log.info("  -testTreebank <name> [filter]: A treebank for testing the model.");
  log.info("  -train: Run training over the treebank, testing on the testTreebank.");
  log.info("  -continueTraining <name>: The name of a file to continue training.");
  log.info("  -nofilter: Rules for the parser will not be filtered based on the training treebank.");
  log.info("  -runGradientCheck: Run a gradient check.");
  log.info("  -resultsRecord: A file for recording info on intermediate results");
  log.info();
  log.info("Options overlapping the parser:");
  log.info("  -trainingThreads <int>: How many threads to use when training.");
  log.info("  -dvKBest <int>: How many hypotheses to use from the underlying parser.");
  log.info("  -trainingIterations <int>: When training, how many times to go through the train set.");
  log.info("  -regCost <double>: How large of a cost to put on regularization.");
  log.info("  -batchSize <int>: How many trees to use in each batch of the training.");
  log.info("  -qnIterationsPerBatch <int>: How many steps to take per batch.");
  log.info("  -qnEstimates <int>: Parameter for qn optimization.");
  log.info("  -qnTolerance <double>: Tolerance for early exit when optimizing a batch.");
  log.info("  -debugOutputFrequency <int>: How frequently to score a model when training and write out intermediate models.");
  log.info("  -maxTrainTimeSeconds <int>: How long to train before terminating.");
  log.info("  -randomSeed <long>: A starting point for the random number generator.  Setting this should lead to repeatable results, even taking into account randomness.  Otherwise, a new random seed will be picked.");
  log.info("  -wordVectorFile <name>: A filename to load word vectors from.");
  log.info("  -numHid: The size of the matrices.  In most circumstances, should be set to the size of the word vectors.");
  log.info("  -learningRate: The rate of optimization when training");
  log.info("  -deltaMargin: How much we punish trees for being incorrect when training");
  log.info("  -(no)unknownNumberVector: Whether or not to use a word vector for unknown numbers");
  log.info("  -(no)unknownDashedWordVectors: Whether or not to split unknown dashed words");
  log.info("  -(no)unknownCapsVector: Whether or not to use a word vector for unknown words with capitals");
  log.info("  -dvSimplifiedModel: Use a greatly dumbed down DVModel");
  log.info("  -scalingForInit: How much to scale matrices when creating a new DVModel");
  log.info("  -baseParserWeight: A weight to give the original LexicalizedParser when testing (0.2 seems to work well for English)");
  log.info("  -unkWord: The vector representing unknown word in the word vectors file");
  log.info("  -transformMatrixType: A couple different methods for initializing transform matrices");
  log.info("  -(no)trainWordVectors: whether or not to train the word vectors along with the matrices.  True by default");
}
