{
  Timing convertTiming=new Timing();
  convertTiming.doing("Converting trees");
  IdentityHashMap<Tree,List<Tree>> topParses=CacheParseHypotheses.convertToTrees(trainingBatch,compressedParses,op.trainOptions.trainingThreads);
  convertTiming.done();
  DVParserCostAndGradient gcFunc=new DVParserCostAndGradient(trainingBatch,topParses,dvModel,op);
  double[] theta=dvModel.paramsToVector();
switch (MINIMIZER) {
case (1):
{
      QNMinimizer qn=new QNMinimizer(op.trainOptions.qnEstimates,true);
      qn.useMinPackSearch();
      qn.useDiagonalScaling();
      qn.terminateOnAverageImprovement(true);
      qn.terminateOnNumericalZero(true);
      qn.terminateOnRelativeNorm(true);
      theta=qn.minimize(gcFunc,op.trainOptions.qnTolerance,theta,op.trainOptions.qnIterationsPerBatch);
      break;
    }
case 2:
{
    double lastCost=0, currCost=0;
    boolean firstTime=true;
    for (int i=0; i < op.trainOptions.qnIterationsPerBatch; i++) {
      double[] grad=gcFunc.derivativeAt(theta);
      currCost=gcFunc.valueAt(theta);
      log.info("batch cost: " + currCost);
      lastCost=currCost;
      ArrayMath.addMultInPlace(theta,grad,-1 * op.trainOptions.learningRate);
    }
    break;
  }
case 3:
{
  double eps=1e-3;
  double currCost=0;
  for (int i=0; i < op.trainOptions.qnIterationsPerBatch; i++) {
    double[] gradf=gcFunc.derivativeAt(theta);
    currCost=gcFunc.valueAt(theta);
    log.info("batch cost: " + currCost);
    for (int feature=0; feature < gradf.length; feature++) {
      sumGradSquare[feature]=sumGradSquare[feature] + gradf[feature] * gradf[feature];
      theta[feature]=theta[feature] - (op.trainOptions.learningRate * gradf[feature] / (Math.sqrt(sumGradSquare[feature]) + eps));
    }
  }
  break;
}
default :
{
throw new IllegalArgumentException("Unsupported minimizer " + MINIMIZER);
}
}
dvModel.vectorToParams(theta);
}
