{
  assert(m1.appearEarlierThan(m2));
  Counter<String> features=new ClassicCounter<>();
  features.incrementCount("BIAS_FEATURE");
  features.incrementCount("docType=" + doc.docType);
  if (doc.docInfo != null && doc.docInfo.containsKey("DOC_ID")) {
    features.incrementCount("doc-source=" + doc.docInfo.get("DOC_ID").split("/")[1]);
  }
  List<String> singletonFeatures1=m1.getSingletonFeatures(dictionaries);
  List<String> singletonFeatures2=m2.getSingletonFeatures(dictionaries);
  for (  Map.Entry<Integer,String> e : SINGLETON_FEATURES.entrySet()) {
    if (e.getKey() < singletonFeatures1.size() && e.getKey() < singletonFeatures2.size()) {
      features.incrementCount(e.getValue() + "=" + singletonFeatures1.get(e.getKey())+ "_"+ singletonFeatures2.get(e.getKey()));
    }
  }
  features.incrementCount("roles=" + getRole(m1) + "_"+ getRole(m2));
  CoreLabel headCL1=headWord(m1);
  CoreLabel headCL2=headWord(m2);
  String headPOS1=getPOS(headCL1);
  String headPOS2=getPOS(headCL2);
  features.incrementCount("head-pos-s=" + headPOS1 + "_"+ headPOS2);
  features.incrementCount("head-words=" + wordIndicator("h_" + headCL1.word().toLowerCase() + "_"+ headCL2.word().toLowerCase(),headPOS1 + "_" + headPOS2));
  addFeature(features,"animacies-agree",m2.animaciesAgree(m1));
  addFeature(features,"attributes-agree",m2.attributesAgree(m1,dictionaries));
  addFeature(features,"entity-types-agree",m2.entityTypesAgree(m1,dictionaries));
  addFeature(features,"numbers-agree",m2.numbersAgree(m1));
  addFeature(features,"genders-agree",m2.gendersAgree(m1));
  addFeature(features,"ner-strings-equal",m1.nerString.equals(m2.nerString));
  addFeature(features,"antecedent-head-in-anaphor",headContainedIn(m1,m2));
  addFeature(features,"anaphor-head-in-antecedent",headContainedIn(m2,m1));
  addFeature(features,"heads-equal",m1.headString.equalsIgnoreCase(m2.headString));
  addFeature(features,"heads-agree",m2.headsAgree(m1));
  if (m1.mentionType == MentionType.PRONOMINAL && m2.mentionType == MentionType.PRONOMINAL && m1.headWord.word().toLowerCase().equals(m2.headWord.word().toLowerCase())) {
    features.incrementCount("pronoun-match");
  }
  if (m1.mentionType != MentionType.PRONOMINAL && m2.mentionType != MentionType.PRONOMINAL) {
    addFeature(features,"antecedent-in-anaphor",m2.spanToString().toLowerCase().contains(m1.spanToString().toLowerCase()));
    addFeature(features,"anaphor-in-antecedent",m1.spanToString().toLowerCase().contains(m2.spanToString().toLowerCase()));
    addFeature(features,"exact-match",m1.toString().trim().toLowerCase().equals(m2.toString().trim().toLowerCase()));
    addFeature(features,"partial-match",partialMatch(m1,m2));
    double editDistance=StringUtils.editDistance(m1.spanToString(),m2.spanToString()) / (double)(m1.spanToString().length() + m2.spanToString().length());
    features.incrementCount("edit-distance",editDistance);
    features.incrementCount("edit-distance=" + ((int)(editDistance * 10) / 10.0));
    double headEditDistance=StringUtils.editDistance(m1.headString,m2.headString) / (double)(m1.headString.length() + m2.headString.length());
    features.incrementCount("head-edit-distance",headEditDistance);
    features.incrementCount("head-edit-distance=" + ((int)(headEditDistance * 10) / 10.0));
  }
  features.incrementCount("mention-distance",m2.mentionNum - m1.mentionNum);
  features.incrementCount("sentence-distance",m2.sentNum - m1.sentNum);
  features.incrementCount("mention-distance=" + bin(m2.mentionNum - m1.mentionNum));
  features.incrementCount("sentence-distance=" + bin(m2.sentNum - m1.sentNum));
  if (m2.sentNum == m1.sentNum) {
    features.incrementCount("word-distance",m2.startIndex - m1.endIndex);
    features.incrementCount("word-distance=" + bin(m2.startIndex - m1.endIndex));
    if (m1.endIndex > m2.startIndex) {
      features.incrementCount("spans-intersect");
    }
  }
  Set<Mention> ms1=new HashSet<>();
  ms1.add(m1);
  Set<Mention> ms2=new HashSet<>();
  ms2.add(m2);
  Random r=new Random();
  CorefCluster c1=new CorefCluster(20000 + r.nextInt(10000),ms1);
  CorefCluster c2=new CorefCluster(10000 + r.nextInt(10000),ms2);
  String s2=m2.lowercaseNormalizedSpanString();
  String s1=m1.lowercaseNormalizedSpanString();
  addFeature(features,"mention-speaker-PER0",m2.headWord.get(SpeakerAnnotation.class).equalsIgnoreCase("PER0"));
  addFeature(features,"antecedent-is-anaphor-speaker",Rules.antecedentIsMentionSpeaker(doc,m2,m1,dictionaries));
  addFeature(features,"person-disagree-same-speaker",Rules.entityPersonDisagree(doc,m2,m1,dictionaries) && Rules.entitySameSpeaker(doc,m2,m1));
  addFeature(features,"antecedent-matches-anaphor-speaker",Rules.antecedentMatchesMentionSpeakerAnnotation(m2,m1,doc));
  addFeature(features,"discourse-you-PER0",m2.person == Person.YOU && doc.docType == DocType.ARTICLE && m2.headWord.get(CoreAnnotations.SpeakerAnnotation.class).equals("PER0"));
  addFeature(features,"same-speaker",m2.speakerInfo != null && m2.speakerInfo == m1.speakerInfo);
  addFeature(features,"1st-person-same-speaker",m2.number == Number.SINGULAR && dictionaries.firstPersonPronouns.contains(s1) && m1.number == Number.SINGULAR && dictionaries.firstPersonPronouns.contains(s2) && Rules.entitySameSpeaker(doc,m2,m1));
  addFeature(features,"1st-person-mention-speaker-match",m2.number == Number.SINGULAR && dictionaries.firstPersonPronouns.contains(s2) && Rules.antecedentIsMentionSpeaker(doc,m2,m1,dictionaries));
  addFeature(features,"1st-person-antecedent-speaker-match",m1.number == Number.SINGULAR && dictionaries.firstPersonPronouns.contains(s1) && Rules.antecedentIsMentionSpeaker(doc,m1,m2,dictionaries));
  addFeature(features,"2nd-person-same-speaker",dictionaries.secondPersonPronouns.contains(s1) && dictionaries.secondPersonPronouns.contains(s2) && Rules.entitySameSpeaker(doc,m2,m1));
  addFeature(features,"discourse-between-two-person",((m2.person == Person.I && m1.person == Person.YOU || (m2.person == Person.YOU && m1.person == Person.I)) && (m2.headWord.get(CoreAnnotations.UtteranceAnnotation.class) - m1.headWord.get(CoreAnnotations.UtteranceAnnotation.class) == 1) && doc.docType == DocType.CONVERSATION));
  addFeature(features,"discourse-match-reflexive-pronoun",dictionaries.reflexivePronouns.contains(m2.headString) && Rules.entitySubjectObject(m2,m1));
  addFeature(features,"incompatible-not-match",m1.person != Person.I && m2.person != Person.I && (Rules.antecedentIsMentionSpeaker(doc,m1,m2,dictionaries) || Rules.antecedentIsMentionSpeaker(doc,m2,m1,dictionaries)));
  int utteranceDist=Math.abs(m1.headWord.get(CoreAnnotations.UtteranceAnnotation.class) - m2.headWord.get(CoreAnnotations.UtteranceAnnotation.class));
  if (doc.docType != DocType.ARTICLE && utteranceDist == 1 && !Rules.entitySameSpeaker(doc,m2,m1)) {
    addFeature(features,"incompatibles-neighbor-I",m1.person == Person.I && m2.person == Person.I);
    addFeature(features,"incompatibles-neighbor-you",m1.person == Person.YOU && m2.person == Person.YOU);
    addFeature(features,"incompatibles-neighbor-we",m1.person == Person.WE && m2.person == Person.WE);
  }
  addFeature(features,"apposition",m1.isApposition(m2));
  String firstWord1=firstWord(m1).word().toLowerCase();
  addFeature(features,"indefinite-article-np",(m1.appositions == null && m1.predicateNominatives == null && (firstWord1.equals("a") || firstWord1.equals("an"))));
  addFeature(features,"indefinite-pronoun",dictionaries.indefinitePronouns.contains(m1.lowercaseNormalizedSpanString()));
  addFeature(features,"indefinite-pronoun-start",dictionaries.indefinitePronouns.contains(firstWord1));
  addFeature(features,"far-this",m2.lowercaseNormalizedSpanString().equals("this") && Math.abs(m2.sentNum - m1.sentNum) > 3);
  addFeature(features,"per0-you-in-article",m2.person == Person.YOU && doc.docType == DocType.ARTICLE && m2.headWord.get(CoreAnnotations.SpeakerAnnotation.class).equals("PER0"));
  addFeature(features,"inside-in",m2.insideIn(m1) || m1.insideIn(m2));
  addFeature(features,"common-noun-poper-noun",m1.mentionType != MentionType.PROPER && (m2.headWord.get(CoreAnnotations.PartOfSpeechAnnotation.class).startsWith("NNP") || !m2.headWord.word().substring(1).equals(m2.headWord.word().substring(1).toLowerCase())));
  addFeature(features,"indefinite-determiners",dictionaries.indefinitePronouns.contains(m1.originalSpan.get(0).lemma()) || dictionaries.indefinitePronouns.contains(m2.originalSpan.get(0).lemma()));
  addFeature(features,"entity-attributes-agree",Rules.entityAttributesAgree(c2,c1));
  addFeature(features,"entity-token-distance",Rules.entityTokenDistance(m2,m1));
  addFeature(features,"same-list",m1.isMemberOfSameList(m2));
  addFeature(features,"same-proper-head-last-word",Rules.entitySameProperHeadLastWord(c2,c1,m2,m1));
  addFeature(features,"i-within-i",Rules.entityIWithinI(m2,m1,dictionaries));
  addFeature(features,"exact-string-match",Rules.entityExactStringMatch(c2,c1,dictionaries,doc.roleSet));
  addFeature(features,"relaxed-string-match",Rules.entityRelaxedExactStringMatch(c2,c1,m2,m1,dictionaries,doc.roleSet));
  addFeature(features,"entity-heads-agree",Rules.entityHeadsAgree(c2,c1,m2,m1,dictionaries));
  addFeature(features,"entity-relaxed-heads-agree",Rules.entityRelaxedHeadsAgreeBetweenMentions(c2,c1,m2,m1));
  addFeature(features,"is-acronym",Rules.entityIsAcronym(doc,c2,c1));
  addFeature(features,"demonym",m2.isDemonym(m1,dictionaries));
  addFeature(features,"incompatible-modifier",Rules.entityHaveIncompatibleModifier(m2,m1));
  addFeature(features,"head-lemma-match",m1.headWord.lemma().equals(m2.headWord.lemma()));
  addFeature(features,"words-included",Rules.entityWordsIncluded(c2,c1,m2,m1));
  addFeature(features,"extra-proper-noun",Rules.entityHaveExtraProperNoun(m2,m1,new HashSet<String>()));
  addFeature(features,"number-in-later-mentions",Rules.entityNumberInLaterMention(m2,m1));
  if (useConstituencyParse) {
    if (m1.sentNum == m2.sentNum) {
      int clauseCount=0;
      Tree tree=m2.contextParseTree;
      Tree current=m2.mentionSubTree;
      while (true) {
        current=current.ancestor(1,tree);
        if (current.label().value().startsWith("S")) {
          clauseCount++;
        }
        if (current.dominates(m1.mentionSubTree)) {
          break;
        }
        if (current.label().value().equals("ROOT") || current.ancestor(1,tree) == null) {
          break;
        }
      }
      features.incrementCount("clause-count",clauseCount);
      features.incrementCount("clause-count=" + bin(clauseCount));
    }
    if (RuleBasedCorefMentionFinder.isPleonastic(m2,m2.contextParseTree) || RuleBasedCorefMentionFinder.isPleonastic(m1,m1.contextParseTree)) {
      features.incrementCount("pleonastic-it");
    }
    if (maximalNp(m1.mentionSubTree) == maximalNp(m2.mentionSubTree)) {
      features.incrementCount("same-maximal-np");
    }
    boolean m1Embedded=headEmbeddingLevel(m1.mentionSubTree,m1.headIndex - m1.startIndex) > 1;
    boolean m2Embedded=headEmbeddingLevel(m2.mentionSubTree,m2.headIndex - m2.startIndex) > 1;
    features.incrementCount("embedding=" + m1Embedded + "_"+ m2Embedded);
  }
  String firstWord2=firstWord(m2).word().toLowerCase();
  if (firstWord2.equals("the")) {
    addFeature(features,"DEF_NP",true);
  }
  if (firstWord2.equals("this") || firstWord2.equals("that") || firstWord2.equals("these")|| firstWord2.equals("those")) {
    addFeature(features,"DEM_NP",true);
  }
  features.remove("mention-distance");
  features.remove("sentence-distance");
  features.remove("word-distance");
  if (features.containsKey("antecedent-head-in-anaphor") || features.containsKey("anaphor-head-in-antecedent")) {
    features.remove("antecedent-head-in-anaphor");
    features.remove("anaphor-head-in-antecedent");
    features.incrementCount("head-match");
  }
  if (m1.spanToString().equals("Orlando police chief Walter uh Zalisko who was on the same cruise that Smith travelled on as well as forensic expert Larry Kobilinsky")) {
    System.out.println(features);
  }
  return features;
}
