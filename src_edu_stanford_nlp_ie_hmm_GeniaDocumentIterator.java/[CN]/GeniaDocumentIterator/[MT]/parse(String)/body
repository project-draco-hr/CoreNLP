{
  TypedTaggedDocument<L> doc=new TypedTaggedDocument<L>(targetFields);
  if (ignoreSubTags) {
    text=proteinCollapser.matcher(text).replaceAll("$1");
    text=dnaCollapser.matcher(text).replaceAll("$1");
    text=rnaCollapser.matcher(text).replaceAll("$1");
  }
  String[] tags=new String[targetFields.length];
  for (int i=1; i < targetFields.length; i++) {
    tags[i]="<cons sem=\"G#" + targetFields[i] + "\">";
  }
  TaggedStreamTokenizer tokenizer=makeTokenizer(new StringReader(text));
  if (tokenizer == null) {
    throw (new IllegalStateException("Unable to create tokenizer"));
  }
  List<Word> words=new ArrayList<Word>();
  try {
    tokenizer.setDiscardHtml(false);
    tokenizer.nextToken();
    if ((tokenizer.ttype == TaggedStreamTokenizer.TT_BACKGROUND_HTML) && tokenizer.sval.startsWith(XMLTAG)) {
      tokenizer.nextToken();
      while (tokenizer.ttype != TaggedStreamTokenizer.TT_EOF) {
        if (tokenizer.ttype == TaggedStreamTokenizer.TT_BACKGROUND_WORD || tokenizer.ttype == TaggedStreamTokenizer.TT_TARGET_WORD) {
          int newtype=-1;
          boolean set=false;
          if (tokenizer.ttype != TaggedStreamTokenizer.TT_TARGET_WORD) {
            newtype=0;
          }
 else {
            for (int t=1; t < targetFields.length && !set; t++) {
              if (tags[t].equals(tokenizer.attr)) {
                set=true;
              }
              newtype=t;
            }
          }
          words.add(new TypedTaggedWord(tokenizer.sval,newtype));
        }
        tokenizer.nextToken();
      }
      doc.addAll(words);
    }
 else {
      tokenizer.setDiscardHtml(true);
      while (tokenizer.ttype != TaggedStreamTokenizer.TT_EOF) {
        if (tokenizer.ttype == TaggedStreamTokenizer.TT_BACKGROUND_WORD || tokenizer.ttype == TaggedStreamTokenizer.TT_TARGET_WORD) {
          int newtype=-1;
          boolean set=false;
          if (tokenizer.ttype != TaggedStreamTokenizer.TT_TARGET_WORD) {
            newtype=0;
          }
 else {
            for (int t=1; t < targetFields.length && !set; t++) {
              if (tags[t].equals(tokenizer.attr)) {
                set=true;
              }
              newtype=t;
            }
          }
          words.add(new TypedTaggedWord(tokenizer.sval,newtype));
        }
        tokenizer.nextToken();
      }
      doc.addAll(words);
    }
  }
 catch (  Exception e) {
    e.printStackTrace();
  }
  return (doc);
}
