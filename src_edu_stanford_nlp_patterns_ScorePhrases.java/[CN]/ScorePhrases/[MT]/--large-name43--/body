{
  TwoDimensionalCounter<CandidatePhrase,E> wordsandLemmaPatExtracted=new TwoDimensionalCounter<CandidatePhrase,E>();
  if (constVars.doNotApplyPatterns) {
    ConstantsAndVariables.DataSentsIterator sentsIter=new ConstantsAndVariables.DataSentsIterator(constVars.batchProcessSents);
    while (sentsIter.hasNext()) {
      Pair<Map<String,DataInstance>,File> sentsf=sentsIter.next();
      this.statsWithoutApplyingPatterns(sentsf.first(),patternsForEachToken,patternsLearnedThisIter,wordsandLemmaPatExtracted);
    }
  }
 else {
    if (patternsLearnedThisIter.size() > 0) {
      this.applyPats(patternsLearnedThisIter,label,wordsandLemmaPatExtracted,matchedTokensByPat);
    }
  }
  if (computeProcDataFreq) {
    if (!phraseScorer.wordFreqNorm.equals(Normalization.NONE)) {
      Redwood.log(Redwood.DBG,"computing processed freq");
      for (      Entry<CandidatePhrase,Double> fq : Data.rawFreq.entrySet()) {
        double in=fq.getValue();
        if (phraseScorer.wordFreqNorm.equals(Normalization.SQRT))         in=Math.sqrt(in);
 else         if (phraseScorer.wordFreqNorm.equals(Normalization.LOG))         in=1 + Math.log(in);
 else         throw new RuntimeException("can't understand the normalization");
        Data.processedDataFreq.setCount(fq.getKey(),in);
      }
    }
 else     Data.processedDataFreq=Data.rawFreq;
  }
  if (constVars.wordScoring.equals(WordScoring.WEIGHTEDNORM)) {
    for (    CandidatePhrase en : wordsandLemmaPatExtracted.firstKeySet()) {
      if (!constVars.getOtherSemanticClassesWords().contains(en.getPhrase()) && !constVars.getOtherSemanticClassesWords().contains(en.getPhraseLemma())) {
        terms.addAll(en,wordsandLemmaPatExtracted.getCounter(en));
      }
      wordsPatExtracted.addAll(en,wordsandLemmaPatExtracted.getCounter(en));
    }
    removeKeys(terms,constVars.getStopWords());
    Counter<CandidatePhrase> phraseScores=phraseScorer.scorePhrases(label,terms,wordsPatExtracted,allSelectedPatterns,alreadyIdentifiedWords,false);
    Set<CandidatePhrase> ignoreWordsAll;
    if (ignoreWords != null && !ignoreWords.isEmpty()) {
      ignoreWordsAll=CollectionUtils.unionAsSet(ignoreWords,constVars.getOtherSemanticClassesWords());
    }
 else     ignoreWordsAll=constVars.getOtherSemanticClassesWords();
    ignoreWordsAll.addAll(constVars.getSeedLabelDictionary().get(label));
    ignoreWordsAll.addAll(constVars.getLearnedWords().get(label).keySet());
    Counter<CandidatePhrase> finalwords=chooseTopWords(phraseScores,terms,phraseScores,ignoreWordsAll,constVars.thresholdWordExtract);
    scoreForAllWordsThisIteration.clear();
    Counters.addInPlace(scoreForAllWordsThisIteration,phraseScores);
    Redwood.log(ConstantsAndVariables.minimaldebug,"\n\n## Selected Words for " + label + " : "+ Counters.toSortedString(finalwords,finalwords.size(),"%1$s:%2$.2f","\t"));
    if (constVars.outDir != null && !constVars.outDir.isEmpty()) {
      String outputdir=constVars.outDir + "/" + identifier+ "/"+ label;
      IOUtils.ensureDir(new File(outputdir));
      TwoDimensionalCounter<CandidatePhrase,CandidatePhrase> reasonForWords=new TwoDimensionalCounter<CandidatePhrase,CandidatePhrase>();
      for (      CandidatePhrase word : finalwords.keySet()) {
        for (        E l : wordsPatExtracted.getCounter(word).keySet()) {
          for (          CandidatePhrase w2 : patternsAndWords4Label.getCounter(l)) {
            reasonForWords.incrementCount(word,w2);
          }
        }
      }
      Redwood.log(ConstantsAndVariables.minimaldebug,"Saving output in " + outputdir);
      String filename=outputdir + "/words.json";
      JsonArrayBuilder obj=Json.createArrayBuilder();
      if (writtenInJustification.containsKey(label) && writtenInJustification.get(label)) {
        JsonReader jsonReader=Json.createReader(new BufferedInputStream(new FileInputStream(filename)));
        JsonArray objarr=jsonReader.readArray();
        for (        JsonValue o : objarr)         obj.add(o);
        jsonReader.close();
      }
      JsonArrayBuilder objThisIter=Json.createArrayBuilder();
      for (      CandidatePhrase w : reasonForWords.firstKeySet()) {
        JsonObjectBuilder objinner=Json.createObjectBuilder();
        JsonArrayBuilder l=Json.createArrayBuilder();
        for (        CandidatePhrase w2 : reasonForWords.getCounter(w).keySet()) {
          l.add(w2.getPhrase());
        }
        JsonArrayBuilder pats=Json.createArrayBuilder();
        for (        E p : wordsPatExtracted.getCounter(w)) {
          pats.add(p.toStringSimple());
        }
        objinner.add("reasonwords",l);
        objinner.add("patterns",pats);
        objinner.add("score",finalwords.getCount(w));
        objinner.add("entity",w.getPhrase());
        objThisIter.add(objinner.build());
      }
      obj.add(objThisIter);
      IOUtils.writeStringToFile(obj.build().toString(),filename,"utf8");
      writtenInJustification.put(label,true);
    }
    if (constVars.justify) {
      Redwood.log(Redwood.DBG,"\nJustification for phrases:\n");
      for (      CandidatePhrase word : finalwords.keySet()) {
        Redwood.log(Redwood.DBG,"Phrase " + word + " extracted because of patterns: \t"+ Counters.toSortedString(wordsPatExtracted.getCounter(word),wordsPatExtracted.getCounter(word).size(),"%1$s:%2$f","\n"));
      }
    }
    return finalwords;
  }
 else   if (constVars.wordScoring.equals(WordScoring.BPB)) {
    Counters.addInPlace(terms,wordsPatExtracted);
    Counter<CandidatePhrase> maxPatWeightTerms=new ClassicCounter<CandidatePhrase>();
    Map<CandidatePhrase,E> wordMaxPat=new HashMap<CandidatePhrase,E>();
    for (    Entry<CandidatePhrase,ClassicCounter<E>> en : terms.entrySet()) {
      Counter<E> weights=new ClassicCounter<E>();
      for (      E k : en.getValue().keySet())       weights.setCount(k,patternsLearnedThisIter.getCount(k));
      maxPatWeightTerms.setCount(en.getKey(),Counters.max(weights));
      wordMaxPat.put(en.getKey(),Counters.argmax(weights));
    }
    Counters.removeKeys(maxPatWeightTerms,alreadyIdentifiedWords);
    double maxvalue=Counters.max(maxPatWeightTerms);
    Set<CandidatePhrase> words=Counters.keysAbove(maxPatWeightTerms,maxvalue - 1e-10);
    CandidatePhrase bestw=null;
    if (words.size() > 1) {
      double max=Double.NEGATIVE_INFINITY;
      for (      CandidatePhrase w : words) {
        if (terms.getCount(w,wordMaxPat.get(w)) > max) {
          max=terms.getCount(w,wordMaxPat.get(w));
          bestw=w;
        }
      }
    }
 else     if (words.size() == 1)     bestw=words.iterator().next();
 else     return new ClassicCounter<CandidatePhrase>();
    Redwood.log(ConstantsAndVariables.minimaldebug,"Selected Words: " + bestw);
    return Counters.asCounter(Arrays.asList(bestw));
  }
 else   throw new RuntimeException("wordscoring " + constVars.wordScoring + " not identified");
}
