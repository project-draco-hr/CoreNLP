{
  if (extractEntities) {
    logger.info("Training entity extraction model(s)");
    if (partition != -1)     logger.info("In partition #" + partition);
    String modelName=serializedEntityExtractorPath;
    if (partition != -1)     modelName+="." + partition;
    File modelFile=new File(modelName);
    logger.fine("forceRetraining = " + forceRetraining + ", modelFile.exists = "+ modelFile.exists());
    if (!forceRetraining && modelFile.exists()) {
      logger.info("Loading entity extraction model from " + modelName + " ...");
      entityExtractor=BasicEntityExtractor.load(modelName,entityClassifier,false);
    }
 else {
      logger.info("Training entity extraction model...");
      entityExtractor=makeEntityExtractor(entityClassifier,entityGazetteerPath);
      entityExtractor.train(training);
      logger.info("Serializing entity extraction model to " + modelName + " ...");
      entityExtractor.save(modelName);
    }
  }
  if (extractRelations) {
    logger.info("Training relation extraction model(s)");
    if (partition != -1)     logger.info("In partition #" + partition);
    String modelName=serializedRelationExtractorPath;
    if (partition != -1)     modelName+="." + partition;
    Annotation predicted=null;
    if (useRelationExtractionModelMerging) {
      String[] modelNames=serializedRelationExtractorPath.split(",");
      if (partition != -1) {
        for (int i=0; i < modelNames.length; i++) {
          modelNames[i]+="." + partition;
        }
      }
      relationExtractor=ExtractorMerger.buildRelationExtractorMerger(modelNames);
    }
 else     if (!forceRetraining && new File(modelName).exists()) {
      logger.info("Loading relation extraction model from " + modelName + " ...");
      relationExtractor=BasicRelationExtractor.load(modelName);
    }
 else {
      RelationFeatureFactory rff=makeRelationFeatureFactory(relationFeatureFactoryClass,relationFeatures,doNotLexicalizeFirstArg);
      if (trainRelationsUsingPredictedEntities) {
        assert(entityExtractor != null);
        predicted=AnnotationUtils.deepMentionCopy(training);
        entityExtractor.annotate(predicted);
        for (        ResultsPrinter rp : entityResultsPrinterSet) {
          String msg=rp.printResults(training,predicted);
          logger.info("Training relation extraction using predicted entitities: entity scores using printer " + rp.getClass() + ":\n"+ msg);
        }
        try {
          changeGoldRelationArgsToPredicted(predicted);
        }
 catch (        Exception e) {
          throw new RuntimeException(e);
        }
      }
      Annotation dataset;
      if (trainRelationsUsingPredictedEntities) {
        dataset=predicted;
      }
 else {
        dataset=training;
      }
      Set<String> relationsToSkip=new HashSet<String>(StringUtils.split(relationsToSkipDuringTraining,","));
      List<List<RelationMention>> backedUpRelations=new ArrayList<List<RelationMention>>();
      if (relationsToSkip.size() > 0) {
        for (        CoreMap sent : dataset.get(CoreAnnotations.SentencesAnnotation.class)) {
          List<RelationMention> relationMentions=sent.get(MachineReadingAnnotations.RelationMentionsAnnotation.class);
          backedUpRelations.add(relationMentions);
        }
        removeSkippableRelations(dataset,relationsToSkip);
      }
      relationExtractor=new BasicRelationExtractor(rff,createUnrelatedRelations,makeRelationMentionFactory(relationMentionFactoryClass));
      Arguments.parse(args,relationExtractor);
      logger.info("Training relation extraction model...");
      relationExtractor.train(dataset);
      logger.info("Serializing relation extraction model to " + modelName + " ...");
      relationExtractor.save(modelName);
      if (relationsToSkip.size() > 0) {
        int sentenceIndex=0;
        for (        CoreMap sentence : dataset.get(CoreAnnotations.SentencesAnnotation.class)) {
          List<RelationMention> relationMentions=backedUpRelations.get(sentenceIndex);
          sentence.set(MachineReadingAnnotations.RelationMentionsAnnotation.class,relationMentions);
          sentenceIndex++;
        }
      }
    }
  }
  if (extractEvents) {
    logger.info("Training event extraction model(s)");
    if (partition != -1)     logger.info("In partition #" + partition);
    String modelName=serializedEventExtractorPath;
    if (partition != -1)     modelName+="." + partition;
    File modelFile=new File(modelName);
    Annotation predicted=null;
    if (!forceRetraining && modelFile.exists()) {
      logger.info("Loading event extraction model from " + modelName + " ...");
      Method mstLoader=(Class.forName("MSTBasedEventExtractor")).getMethod("load",String.class);
      eventExtractor=(Extractor)mstLoader.invoke(null,modelName);
    }
 else {
      if (trainEventsUsingPredictedEntities) {
        assert(entityExtractor != null);
        predicted=AnnotationUtils.deepMentionCopy(training);
        entityExtractor.annotate(predicted);
        for (        ResultsPrinter rp : entityResultsPrinterSet) {
          String msg=rp.printResults(training,predicted);
          logger.info("Training event extraction using predicted entitities: entity scores using printer " + rp.getClass() + ":\n"+ msg);
        }
      }
      Constructor<?> mstConstructor=(Class.forName("edu.stanford.nlp.ie.machinereading.MSTBasedEventExtractor")).getConstructor(boolean.class);
      eventExtractor=(Extractor)mstConstructor.newInstance(trainEventsUsingPredictedEntities);
      logger.info("Training event extraction model...");
      if (trainRelationsUsingPredictedEntities) {
        eventExtractor.train(predicted);
      }
 else {
        eventExtractor.train(training);
      }
      logger.info("Serializing event extraction model to " + modelName + " ...");
      eventExtractor.save(modelName);
    }
  }
}
