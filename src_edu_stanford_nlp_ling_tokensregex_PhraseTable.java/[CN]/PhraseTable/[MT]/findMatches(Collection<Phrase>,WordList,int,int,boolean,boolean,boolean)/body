{
  if (needNormalization) {
    assert(tokenStart >= 0);
    assert(tokenEnd > tokenStart);
    int n=tokenEnd - tokenStart;
    List<String> normalized=new ArrayList<String>(n);
    int[] tokenIndexMap=new int[n + 1];
    int j=0, last=0;
    for (int i=tokenStart; i < tokenEnd; i++) {
      String word=tokens.getWord(i);
      word=getNormalizedForm(word);
      if (word.length() != 0) {
        normalized.add(word);
        tokenIndexMap[j]=i;
        last=i;
        j++;
      }
    }
    tokenIndexMap[j]=Math.min(last + 1,tokenEnd);
    List<PhraseMatch> matched=findMatchesNormalized(acceptablePhrases,new StringList(normalized),0,normalized.size(),findAll,matchEnd);
    for (    PhraseMatch pm : matched) {
      assert(pm.tokenBegin >= 0);
      assert(pm.tokenEnd >= pm.tokenBegin);
      assert(pm.tokenEnd <= normalized.size());
      if (pm.tokenEnd > 0 && pm.tokenEnd > pm.tokenBegin) {
        pm.tokenEnd=tokenIndexMap[pm.tokenEnd - 1] + 1;
      }
 else {
        pm.tokenEnd=tokenIndexMap[pm.tokenEnd];
      }
      pm.tokenBegin=tokenIndexMap[pm.tokenBegin];
      assert(pm.tokenBegin >= 0);
      assert(pm.tokenEnd >= pm.tokenBegin);
    }
    return matched;
  }
 else {
    return findMatchesNormalized(acceptablePhrases,tokens,tokenStart,tokenEnd,findAll,matchEnd);
  }
}
