{
  System.err.print("Learning bigram model...");
  unigramCounter=new ClassicCounter<String>();
  bigramCounter=new TwoDimensionalCounter<String,String>();
  index=new HashIndex<Object>();
  Collection c=new FileSequentialCollection(new File(trainPath),fileExtension,true);
  ReaderIteratorFactory rif=new ReaderIteratorFactory(c);
  ObjectBank bank=new ObjectBank(rif,PTBTokenizer.PTBTokenizerFactory.newPTBTokenizerFactory(true));
  String lastToken=null;
  int i=0;
  for (Iterator tokIter=bank.iterator(); tokIter.hasNext(); ) {
    String thisToken=((Word)tokIter.next()).word();
    thisToken=thisToken.toLowerCase();
    index.add(thisToken);
    unigramCounter.incrementCount(thisToken);
    bigramCounter.incrementCount(lastToken,thisToken);
    lastToken=thisToken;
    i++;
    if (i % 10000 == 0)     System.err.println("Processed " + i);
  }
  totalCount=unigramCounter.totalCount();
  conditionalProbs=new double[unigramCounter.size()];
  System.err.println("done. numTokens=" + i + " vocabSize="+ unigramCounter.size());
  possibleValues=new int[index.size()];
  for (int j=0; j < index.size(); j++) {
    possibleValues[j]=j;
  }
}
