{
  String SerializedParserPath=application.getRealPath("/WEB-INF/data") + File.separator + nameToParserSer.get(parser);
  ParserPack pp=new ParserPack();
  pp.escaper=(Function<List<HasWord>,List<HasWord>>)Class.forName(nameToEscaper.get(parser)).newInstance();
  pp.parser=LexicalizedParser.loadModel(SerializedParserPath);
  pp.tLP=pp.parser.getOp().tlpParams.treebankLanguagePack();
  pp.tagPrint=new TreePrint("wordsAndTags",pp.tLP);
  pp.pennPrint=new TreePrint("penn",pp.tLP);
  if (!parser.equals("Arabic")) {
    pp.typDepPrint=new TreePrint("typedDependencies","basicDependencies",pp.tLP);
    pp.typDepColPrint=new TreePrint("typedDependencies",pp.tLP);
  }
  if (parser.equals("Chinese")) {
    Properties props=new Properties();
    String dataDir=application.getRealPath("/WEB-INF/data/chinesesegmenter");
    CRFClassifier classifier=new CRFClassifier(props);
    BufferedInputStream bis=new BufferedInputStream(new GZIPInputStream(new FileInputStream(dataDir + File.separator + "05202008-ctb6.processed-chris6.lex.gz")));
    classifier.loadClassifier(bis,null);
    bis.close();
    SeqClassifierFlags flags=classifier.flags;
    flags.sighanCorporaDict=dataDir;
    flags.normalizationTable=dataDir + File.separator + "norm.simp.utf8";
    flags.normTableEncoding="UTF-8";
    flags.inputEncoding="UTF-8";
    flags.keepAllWhitespaces=true;
    flags.keepEnglishWhitespaces=true;
    flags.sighanPostProcessing=true;
    pp.segmenter=classifier;
  }
  List<String> defaultQueryPieces;
  if (pp.segmenter != null) {
    defaultQueryPieces=pp.segmenter.segmentString(defaultQuery.get(parser));
  }
 else {
    defaultQueryPieces=Arrays.asList(defaultQuery.get(parser).split("\\s+"));
  }
  List<HasWord> defaultQueryWords=new ArrayList<HasWord>();
  for (  String s : defaultQueryPieces) {
    defaultQueryWords.add(new Word(s));
  }
  pp.parser.parseTree(defaultQueryWords);
  return pp;
}
