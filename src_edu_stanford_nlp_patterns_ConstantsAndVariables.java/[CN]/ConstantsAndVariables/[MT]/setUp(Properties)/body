{
  if (alreadySetUp) {
    return;
  }
  Redwood.log(Redwood.DBG,"Setting up ConstantsAndVariables");
  ArgumentParser.fillOptions(this,props);
  ArgumentParser.fillOptions(PatternFactory.class,props);
  ArgumentParser.fillOptions(SurfacePatternFactory.class,props);
  ArgumentParser.fillOptions(DepPatternFactory.class,props);
  if (wordIgnoreRegex != null && !wordIgnoreRegex.isEmpty()) {
    Redwood.log(Redwood.DBG,"Ignore word regex is " + wordIgnoreRegex);
    PatternFactory.ignoreWordRegex=Pattern.compile(wordIgnoreRegex);
  }
  for (  String label : labels) {
    env.put(label,TokenSequencePattern.getNewEnv());
    for (    Entry<String,Class<? extends Key<String>>> en : this.answerClass.entrySet()) {
      env.get(label).bind(en.getKey(),en.getValue());
    }
    for (    Entry<String,Class> en : generalizeClasses.entrySet())     env.get(label).bind(en.getKey(),en.getValue());
  }
  Redwood.log(Redwood.DBG,channelNameLogger,"Running with debug output");
  stopWords=new HashSet<>();
  if (stopWordsPatternFiles != null) {
    Redwood.log(ConstantsAndVariables.minimaldebug,channelNameLogger,"Reading stop words from " + stopWordsPatternFiles);
    for (    String stopwfile : stopWordsPatternFiles.split("[;,]")) {
      for (      String word : IOUtils.readLines(stopwfile)) {
        if (!word.trim().isEmpty())         stopWords.add(CandidatePhrase.createOrGet(word.trim()));
      }
    }
  }
  englishWords=new HashSet<>();
  if (englishWordsFiles != null) {
    System.out.println("Reading english words from " + englishWordsFiles);
    for (    String englishWordsFile : englishWordsFiles.split("[;,]"))     englishWords.addAll(IOUtils.linesFromFile(englishWordsFile));
  }
  if (commonWordsPatternFiles != null) {
    commonEngWords=Collections.synchronizedSet(new HashSet<>());
    for (    String file : commonWordsPatternFiles.split("[;,]"))     commonEngWords.addAll(IOUtils.linesFromFile(file));
  }
  if (otherSemanticClassesFiles != null) {
    if (otherSemanticClassesWords == null)     otherSemanticClassesWords=Collections.synchronizedSet(new HashSet<>());
    for (    String file : otherSemanticClassesFiles.split("[;,]")) {
      for (      File f : listFileIncludingItself(file)) {
        for (        String w : IOUtils.readLines(f)) {
          String[] t=w.split("\\s+");
          if (t.length <= PatternFactory.numWordsCompoundMax)           otherSemanticClassesWords.add(CandidatePhrase.createOrGet(w));
        }
      }
    }
    System.out.println("Size of othersemantic class variables is " + otherSemanticClassesWords.size());
  }
 else {
    otherSemanticClassesWords=Collections.synchronizedSet(new HashSet<>());
    System.out.println("Size of othersemantic class variables is " + 0);
  }
  String stopStr="/";
  int i=0;
  for (  CandidatePhrase s : stopWords) {
    if (i > 0)     stopStr+="|";
    stopStr+=Pattern.quote(s.getPhrase().replaceAll("\\\\","\\\\\\\\"));
    i++;
  }
  stopStr+="/";
  for (  String label : labels) {
    env.get(label).bind("$FILLER","/" + StringUtils.join(PatternFactory.fillerWords,"|") + "/");
    env.get(label).bind("$STOPWORD",stopStr);
    env.get(label).bind("$MOD","[{tag:/JJ.*/}]");
    if (matchLowerCaseContext) {
      env.get(label).setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE);
      env.get(label).setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE);
    }
    env.get(label).bind("OTHERSEM",PatternsAnnotations.OtherSemanticLabel.class);
    env.get(label).bind("grandparentparsetag",CoreAnnotations.GrandparentAnnotation.class);
  }
  if (wordClassClusterFile != null) {
    wordClassClusters=new HashMap<>();
    for (    String line : IOUtils.readLines(wordClassClusterFile)) {
      String[] t=line.split("\t");
      wordClassClusters.put(t[0],Integer.parseInt(t[1]));
    }
  }
  if (generalWordClassClusterFile != null) {
    setGeneralWordClassClusters(new HashMap<>());
    for (    String line : IOUtils.readLines(generalWordClassClusterFile)) {
      String[] t=line.split("\t");
      getGeneralWordClassClusters().put(t[0],Integer.parseInt(t[1]));
    }
  }
  if (targetAllowedTagsInitialsStr != null) {
    allowedTagsInitials=new HashMap<>();
    for (    String labelstr : targetAllowedTagsInitialsStr.split(";")) {
      String[] t=labelstr.split(",");
      Set<String> st=new HashSet<>();
      for (int j=1; j < t.length; j++)       st.add(t[j]);
      allowedTagsInitials.put(t[0],st);
    }
  }
  if (PatternFactory.useTargetNERRestriction && targetAllowedNERs != null) {
    allowedNERsforLabels=new HashMap<>();
    for (    String labelstr : targetAllowedNERs.split(";")) {
      String[] t=labelstr.split(",");
      Set<String> st=new HashSet<>();
      for (int j=1; j < t.length; j++)       st.add(t[j]);
      allowedNERsforLabels.put(t[0],st);
    }
  }
  for (  String label : labels) {
    learnedWordsEachIter.put(label,new TreeMap<>());
  }
  if (usePhraseEvalGoogleNgram || usePatternEvalDomainNgram) {
    Data.usingGoogleNgram=true;
    ArgumentParser.fillOptions(GoogleNGramsSQLBacked.class,props);
  }
  if (goldEntitiesEvalFiles != null && evaluate)   goldEntities=readGoldEntities(goldEntitiesEvalFiles);
  alreadySetUp=true;
}
