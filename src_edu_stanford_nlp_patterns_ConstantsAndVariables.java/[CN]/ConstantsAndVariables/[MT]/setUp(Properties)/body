{
  if (alreadySetUp) {
    return;
  }
  Execution.fillOptions(this,props);
  Execution.fillOptions(PatternFactory.class,props);
  Execution.fillOptions(SurfacePatternFactory.class,props);
  Execution.fillOptions(DepPatternFactory.class,props);
  if (wordIgnoreRegex != null && !wordIgnoreRegex.isEmpty())   PatternFactory.ignoreWordRegex=Pattern.compile(wordIgnoreRegex);
  for (  String label : labels) {
    env.put(label,TokenSequencePattern.getNewEnv());
    for (    Entry<String,Class<? extends Key<String>>> en : this.answerClass.entrySet()) {
      env.get(label).bind(en.getKey(),en.getValue());
    }
    for (    Entry<String,Class> en : generalizeClasses.entrySet())     env.get(label).bind(en.getKey(),en.getValue());
  }
  Redwood.log(Redwood.DBG,channelNameLogger,"Running with debug output");
  stopWords=new HashSet<CandidatePhrase>();
  if (stopWordsPatternFiles != null) {
    Redwood.log(ConstantsAndVariables.minimaldebug,channelNameLogger,"Reading stop words from " + stopWordsPatternFiles);
    for (    String stopwfile : stopWordsPatternFiles.split("[;,]"))     stopWords.addAll(CandidatePhrase.convertStringPhrases(IOUtils.linesFromFile(stopwfile)));
  }
  englishWords=new HashSet<String>();
  if (englishWordsFiles != null) {
    System.out.println("Reading english words from " + englishWordsFiles);
    for (    String englishWordsFile : englishWordsFiles.split("[;,]"))     englishWords.addAll(IOUtils.linesFromFile(englishWordsFile));
  }
  if (commonWordsPatternFiles != null) {
    commonEngWords=Collections.synchronizedSet(new HashSet<String>());
    for (    String file : commonWordsPatternFiles.split("[;,]"))     commonEngWords.addAll(IOUtils.linesFromFile(file));
  }
  if (otherSemanticClassesFiles != null) {
    if (otherSemanticClassesWords == null)     otherSemanticClassesWords=Collections.synchronizedSet(new HashSet<CandidatePhrase>());
    for (    String file : otherSemanticClassesFiles.split("[;,]")) {
      for (      String w : IOUtils.linesFromFile(file)) {
        String[] t=w.split("\\s+");
        if (t.length <= PatternFactory.numWordsCompound)         otherSemanticClassesWords.add(CandidatePhrase.createOrGet(w));
      }
    }
    System.out.println("Size of othersemantic class variables is " + otherSemanticClassesWords.size());
  }
 else {
    otherSemanticClassesWords=Collections.synchronizedSet(new HashSet<CandidatePhrase>());
    System.out.println("Size of othersemantic class variables is " + 0);
  }
  String stopStr="/";
  int i=0;
  for (  CandidatePhrase s : stopWords) {
    if (i > 0)     stopStr+="|";
    stopStr+=Pattern.quote(s.getPhrase().replaceAll("\\\\","\\\\\\\\"));
    i++;
  }
  stopStr+="/";
  for (  String label : labels) {
    env.get(label).bind("$FILLER","/" + StringUtils.join(PatternFactory.fillerWords,"|") + "/");
    env.get(label).bind("$STOPWORD",stopStr);
    env.get(label).bind("$MOD","[{tag:/JJ.*/}]");
    if (matchLowerCaseContext) {
      env.get(label).setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE);
      env.get(label).setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE);
    }
    env.get(label).bind("OTHERSEM",PatternsAnnotations.OtherSemanticLabel.class);
    env.get(label).bind("grandparentparsetag",CoreAnnotations.GrandparentAnnotation.class);
  }
  if (wordClassClusterFile != null) {
    wordClassClusters=new HashMap<String,Integer>();
    for (    String line : IOUtils.readLines(wordClassClusterFile)) {
      String[] t=line.split("\t");
      wordClassClusters.put(t[0],Integer.parseInt(t[1]));
    }
  }
  if (generalWordClassClusterFile != null) {
    setGeneralWordClassClusters(new HashMap<String,Integer>());
    for (    String line : IOUtils.readLines(generalWordClassClusterFile)) {
      String[] t=line.split("\t");
      getGeneralWordClassClusters().put(t[0],Integer.parseInt(t[1]));
    }
  }
  if (targetAllowedTagsInitialsStr != null) {
    allowedTagsInitials=new HashMap<String,Set<String>>();
    for (    String labelstr : targetAllowedTagsInitialsStr.split(";")) {
      String[] t=labelstr.split(",");
      Set<String> st=new HashSet<String>();
      for (int j=1; j < t.length; j++)       st.add(t[j]);
      allowedTagsInitials.put(t[0],st);
    }
  }
  if (PatternFactory.useTargetNERRestriction && targetAllowedNERs != null) {
    allowedNERsforLabels=new HashMap<String,Set<String>>();
    for (    String labelstr : targetAllowedNERs.split(";")) {
      String[] t=labelstr.split(",");
      Set<String> st=new HashSet<String>();
      for (int j=1; j < t.length; j++)       st.add(t[j]);
      allowedNERsforLabels.put(t[0],st);
    }
  }
  for (  String label : labels) {
    learnedWords.put(label,new ClassicCounter<CandidatePhrase>());
  }
  if (usePhraseEvalGoogleNgram || usePatternEvalDomainNgram) {
    Data.usingGoogleNgram=true;
    Execution.fillOptions(GoogleNGramsSQLBacked.class,props);
  }
  if (goldEntitiesEvalFiles != null)   goldEntities=readGoldEntities(goldEntitiesEvalFiles);
  alreadySetUp=true;
}
