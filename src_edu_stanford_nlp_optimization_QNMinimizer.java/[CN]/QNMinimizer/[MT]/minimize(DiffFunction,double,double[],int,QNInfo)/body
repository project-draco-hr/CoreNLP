{
  if (mem > 0) {
    sayln("QNMinimizer called on double function of " + dFunction.domainDimension() + " variables, using M = "+ mem+ '.');
  }
 else {
    sayln("QNMinimizer called on double function of " + dFunction.domainDimension() + " variables, using dynamic setting of M.");
  }
  if (qn == null && presetInfo == null) {
    qn=new QNInfo(mem);
    noHistory=true;
  }
 else   if (presetInfo != null) {
    qn=presetInfo;
    noHistory=false;
  }
 else   if (qn != null) {
    noHistory=false;
  }
  its=0;
  fevals=0;
  success=false;
  qn.scaleOpt=scaleOpt;
  double[] x=initial;
  double[] rawGrad=new double[x.length];
  double[] newGrad=new double[x.length];
  double[] newX=new double[x.length];
  double[] dir=new double[x.length];
  double value=evaluateFunction(dFunction,x,rawGrad);
  double[] grad;
  if (useOWLQN) {
    double norm=l1NormOWL(x,dFunction);
    value+=norm * lambdaOWL;
    grad=pseudoGradientOWL(x,rawGrad,dFunction);
  }
 else {
    grad=rawGrad;
  }
  PrintWriter outFile=null;
  PrintWriter infoFile=null;
  if (outputToFile) {
    try {
      String baseName="QN_m" + mem + '_'+ lsOpt.toString()+ '_'+ scaleOpt.toString();
      outFile=new PrintWriter(new FileOutputStream(baseName + ".output"),true);
      infoFile=new PrintWriter(new FileOutputStream(baseName + ".info"),true);
      infoFile.println(dFunction.domainDimension() + "; DomainDimension ");
      infoFile.println(mem + "; memory");
    }
 catch (    IOException e) {
      throw new RuntimeIOException("Caught IOException outputting QN data to file",e);
    }
  }
  Record rec=new Record(monitor,functionTolerance,outFile);
  rec.start(value,rawGrad,x);
  maxFevals=(maxFunctionEvaluations > 0) ? maxFunctionEvaluations : Integer.MAX_VALUE;
  sayln("               An explanation of the output:");
  sayln("Iter           The number of iterations");
  sayln("evals          The number of function evaluations");
  sayln("SCALING        <D> Diagonal scaling was used; <I> Scaled Identity");
  sayln("LINESEARCH     [## M steplength]  Minpack linesearch");
  sayln("                   1-Function value was too high");
  sayln("                   2-Value ok, gradient positive, positive curvature");
  sayln("                   3-Value ok, gradient negative, positive curvature");
  sayln("                   4-Value ok, gradient negative, negative curvature");
  sayln("               [.. B]  Backtracking");
  sayln("VALUE          The current function value");
  sayln("TIME           Total elapsed time");
  sayln("|GNORM|        The current norm of the gradient");
  sayln("{RELNORM}      The ratio of the current to initial gradient norms");
  sayln("AVEIMPROVE     The average improvement / current value");
  sayln("EVALSCORE      The last available eval score");
  sayln();
  sayln("Iter ## evals ## <SCALING> [LINESEARCH] VALUE TIME |GNORM| {RELNORM} AVEIMPROVE EVALSCORE");
  StringBuilder sb=new StringBuilder();
  eState state=eState.CONTINUE;
  do {
    try {
      if (!quiet) {
        sayln(sb.toString());
      }
      sb=new StringBuilder();
      boolean doEval=(its >= 0 && its >= startEvaluateIters && evaluateIters > 0 && its % evaluateIters == 0);
      its+=1;
      double newValue;
      sb.append("Iter ").append(its).append(" evals ").append(fevals).append(' ');
      sb.append('<');
      computeDir(dir,grad,x,qn,dFunction,sb);
      sb.append("> ");
      boolean hasNaNDir=false;
      boolean hasNaNGrad=false;
      for (int i=0; i < dir.length; i++) {
        if (dir[i] != dir[i])         hasNaNDir=true;
        if (grad[i] != grad[i])         hasNaNGrad=true;
      }
      if (hasNaNDir && !hasNaNGrad) {
        sayln("(NaN dir likely due to Hessian approx - resetting) ");
        qn.clear();
        sb.append('<');
        computeDir(dir,grad,x,qn,dFunction,sb);
        sb.append("> ");
      }
      sb.append('[');
      double[] newPoint;
      if (useOWLQN) {
        newPoint=lineSearchBacktrackOWL(dFunction,dir,x,newX,grad,value,sb);
        sb.append('B');
      }
 else {
switch (lsOpt) {
case BACKTRACK:
          newPoint=lineSearchBacktrack(dFunction,dir,x,newX,grad,value,sb);
        sb.append('B');
      break;
case MINPACK:
    newPoint=lineSearchMinPack(dFunction,dir,x,newX,grad,value,functionTolerance,sb);
  sb.append('M');
break;
default :
throw new IllegalArgumentException("Invalid line search option for QNMinimizer.");
}
}
newValue=newPoint[f];
sb.append(' ');
sb.append(nf.format(newPoint[a]));
sb.append("] ");
System.arraycopy(dFunction.derivativeAt(newX),0,newGrad,0,newGrad.length);
qn.update(newX,x,newGrad,rawGrad,newPoint[a]);
if (useOWLQN) {
System.arraycopy(newGrad,0,rawGrad,0,newGrad.length);
newGrad=pseudoGradientOWL(newX,newGrad,dFunction);
}
double evalScore=Double.NEGATIVE_INFINITY;
if (doEval) {
evalScore=doEvaluation(newX);
}
rec.add(newValue,newGrad,newX,fevals,evalScore,sb);
if (iterCallbackFunction != null) {
iterCallbackFunction.callback(newX,its,newValue,newGrad);
}
value=newValue;
System.arraycopy(newX,0,x,0,x.length);
System.arraycopy(newGrad,0,grad,0,newGrad.length);
if (fevals > maxFevals) {
throw new MaxEvaluationsExceeded("Exceeded in minimize() loop.");
}
}
 catch (SurpriseConvergence s) {
sayln("QNMinimizer aborted due to surprise convergence");
break;
}
catch (MaxEvaluationsExceeded m) {
sayln("QNMinimizer aborted due to maximum number of function evaluations");
sayln(m.toString());
sayln("** This is not an acceptable termination of QNMinimizer, consider");
sayln("** increasing the max number of evaluations, or safeguarding your");
sayln("** program by checking the QNMinimizer.wasSuccessful() method.");
break;
}
catch (OutOfMemoryError oome) {
if (!qn.s.isEmpty()) {
qn.s.remove(0);
qn.y.remove(0);
qn.rho.remove(0);
sb.append("{Caught OutOfMemory, changing m from ").append(qn.mem).append(" to ").append(qn.s.size()).append("}]");
qn.mem=qn.s.size();
}
 else {
throw oome;
}
}
}
 while ((state=rec.toContinue(sb)) == eState.CONTINUE);
if (evaluateIters > 0) {
double evalScore=(useEvalImprovement ? doEvaluation(rec.getBest()) : doEvaluation(x));
sayln("final evalScore is: " + evalScore);
}
switch (state) {
case TERMINATE_GRADNORM:
sayln("QNMinimizer terminated due to numerically zero gradient: |g| < EPS  max(1,|x|) ");
success=true;
break;
case TERMINATE_RELATIVENORM:
sayln("QNMinimizer terminated due to sufficient decrease in gradient norms: |g|/|g0| < TOL ");
success=true;
break;
case TERMINATE_AVERAGEIMPROVE:
sayln("QNMinimizer terminated due to average improvement: | newest_val - previous_val | / |newestVal| < TOL ");
success=true;
break;
case TERMINATE_MAXITR:
sayln("QNMinimizer terminated due to reached max iteration " + maxItr);
success=true;
break;
case TERMINATE_EVALIMPROVE:
sayln("QNMinimizer terminated due to no improvement on eval ");
success=true;
x=rec.getBest();
break;
default :
log.warn("QNMinimizer terminated without converging");
success=false;
break;
}
double completionTime=rec.howLong();
sayln("Total time spent in optimization: " + nfsec.format(completionTime) + 's');
if (outputToFile) {
infoFile.println(completionTime + "; Total Time ");
infoFile.println(fevals + "; Total evaluations");
infoFile.close();
outFile.close();
}
qn.free();
return x;
}
