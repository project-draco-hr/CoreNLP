{
  say("QNMinimizer called on double function of " + dfunction.domainDimension() + " variables,");
  if (mem > 0) {
    sayln(" using M = " + mem + ".");
  }
 else {
    sayln(" using dynamic setting of M.");
  }
  if (qn == null && presetInfo == null) {
    qn=new QNInfo(mem);
    noHistory=true;
  }
 else   if (presetInfo != null) {
    qn=presetInfo;
    noHistory=false;
  }
 else   if (qn != null) {
    noHistory=false;
  }
  double[] x, newX, grad, newGrad, dir;
  double value;
  its=0;
  fevals=0;
  success=false;
  qn.scaleOpt=scaleOpt;
  x=initial;
  grad=new double[x.length];
  newGrad=new double[x.length];
  newX=new double[x.length];
  dir=new double[x.length];
  value=evaluateFunction(dfunction,x,grad);
  if (useOWLQN) {
    double norm=l1NormOWL(x,dfunction);
    value+=norm * lambdaOWL;
    grad=pseudoGradientOWL(x,grad,dfunction);
  }
  PrintWriter outFile=null;
  PrintWriter infoFile=null;
  if (outputToFile) {
    try {
      String baseName="QN_m" + mem + "_"+ lsOpt.toString()+ "_"+ scaleOpt.toString();
      outFile=new PrintWriter(new FileOutputStream(baseName + ".output"),true);
      infoFile=new PrintWriter(new FileOutputStream(baseName + ".info"),true);
      infoFile.println(dfunction.domainDimension() + "; DomainDimension ");
      infoFile.println(mem + "; memory");
    }
 catch (    IOException e) {
      throw new RuntimeIOException("Caught IOException outputting QN data to file",e);
    }
  }
  Record rec=new Record(quiet,monitor,functionTolerance,outFile);
  rec.start(value,grad,x);
  maxFevals=(maxFunctionEvaluations > 0) ? maxFunctionEvaluations : Integer.MAX_VALUE;
  sayln("               An explanation of the output:");
  sayln("Iter           The number of iterations");
  sayln("evals          The number of function evaluations");
  sayln("SCALING        <D> Diagonal scaling was used; <I> Scaled Identity");
  sayln("LINESEARCH     [## M steplength]  Minpack linesearch");
  sayln("                   1-Function value was too high");
  sayln("                   2-Value ok, gradient positive, positive curvature");
  sayln("                   3-Value ok, gradient negative, positive curvature");
  sayln("                   4-Value ok, gradient negative, negative curvature");
  sayln("               [.. B]  Backtracking");
  sayln("VALUE          The current function value");
  sayln("TIME           Total elapsed time");
  sayln("|GNORM|        The current norm of the gradient");
  sayln("{RELNORM}      The ratio of the current to initial gradient norms");
  sayln("AVEIMPROVE     The average improvement / current value");
  sayln("EVALSCORE      The last available eval score");
  sayln();
  sayln("Iter ## evals ## <SCALING> [LINESEARCH] VALUE TIME |GNORM| {RELNORM} AVEIMPROVE EVALSCORE");
  do {
    try {
      sayln();
      boolean doEval=(its >= 0 && its >= startEvaluateIters && evaluateIters > 0 && its % evaluateIters == 0);
      its+=1;
      double newValue;
      double[] newPoint=new double[3];
      say("Iter " + its + " evals "+ fevals+ " ");
      say("<");
      computeDir(dir,grad,x,qn,dfunction);
      say("> ");
      boolean hasNaNDir=false;
      boolean hasNaNGrad=false;
      for (int i=0; i < dir.length; i++) {
        if (dir[i] != dir[i])         hasNaNDir=true;
        if (grad[i] != grad[i])         hasNaNGrad=true;
      }
      if (hasNaNDir && !hasNaNGrad) {
        say("(NaN dir likely due to Hessian approx - resetting) ");
        qn.clear();
        say("<");
        computeDir(dir,grad,x,qn,dfunction);
        say("> ");
      }
      say("[");
      if (useOWLQN) {
        newPoint=lineSearchBacktrackOWL(dfunction,dir,x,newX,grad,value);
        say("B");
      }
 else {
switch (lsOpt) {
case BACKTRACK:
          newPoint=lineSearchBacktrack(dfunction,dir,x,newX,grad,value);
        say("B");
      break;
case MINPACK:
    newPoint=lineSearchMinPack(dfunction,dir,x,newX,grad,value,functionTolerance);
  say("M");
break;
default :
sayln("Invalid line search option for QNMinimizer. ");
System.exit(1);
break;
}
}
newValue=newPoint[f];
System.err.print(" " + nf.format(newPoint[a]));
say("] ");
System.arraycopy(dfunction.derivativeAt(newX),0,newGrad,0,newGrad.length);
qn.update(newX,x,newGrad,grad,newPoint[a]);
if (useOWLQN) {
newGrad=pseudoGradientOWL(newX,newGrad,dfunction);
}
double evalScore=Double.NEGATIVE_INFINITY;
if (doEval) {
evalScore=doEvaluation(newX);
}
rec.add(newValue,newGrad,newX,fevals,evalScore);
value=newValue;
System.arraycopy(newX,0,x,0,x.length);
System.arraycopy(newGrad,0,grad,0,newGrad.length);
if (quiet) {
System.err.print(".");
}
if (fevals > maxFevals) {
throw new MaxEvaluationsExceeded(" Exceeded in minimize() loop ");
}
}
 catch (SurpriseConvergence s) {
sayln();
sayln("QNMinimizer aborted due to surprise convergence");
break;
}
catch (MaxEvaluationsExceeded m) {
sayln();
sayln("QNMinimizer aborted due to maximum number of function evaluations");
sayln(m.toString());
sayln("** This is not an acceptable termination of QNMinimizer, consider");
sayln("** increasing the max number of evaluations, or safeguarding your");
sayln("** program by checking the QNMinimizer.wasSuccessful() method.");
break;
}
catch (OutOfMemoryError oome) {
sayln();
if (!qn.s.isEmpty()) {
qn.s.remove(0);
qn.y.remove(0);
qn.rho.remove(0);
qn.mem=qn.s.size();
System.err.println("Caught OutOfMemoryError, changing m = " + qn.mem);
}
 else {
throw oome;
}
}
}
 while ((state=rec.toContinue()) == eState.CONTINUE);
if (evaluateIters > 0) {
double evalScore=(useEvalImprovement ? doEvaluation(rec.getBest()) : doEvaluation(x));
sayln("final evalScore is: " + evalScore);
}
System.err.println();
switch (state) {
case TERMINATE_GRADNORM:
System.err.println("QNMinimizer terminated due to numerically zero gradient: |g| < EPS  max(1,|x|) ");
success=true;
break;
case TERMINATE_RELATIVENORM:
System.err.println("QNMinimizer terminated due to sufficient decrease in gradient norms: |g|/|g0| < TOL ");
success=true;
break;
case TERMINATE_AVERAGEIMPROVE:
System.err.println("QNMinimizer terminated due to average improvement: | newest_val - previous_val | / |newestVal| < TOL ");
success=true;
break;
case TERMINATE_MAXITR:
System.err.println("QNMinimizer terminated due to reached max iteration " + maxItr);
success=true;
break;
case TERMINATE_EVALIMPROVE:
System.err.println("QNMinimizer terminated due to no improvement on eval ");
success=true;
x=rec.getBest();
break;
default :
System.err.println("QNMinimizer terminated without converging");
success=false;
break;
}
double completionTime=rec.howLong();
sayln("Total time spent in optimization: " + nfsec.format(completionTime) + "s");
if (outputToFile) {
infoFile.println(completionTime + "; Total Time ");
infoFile.println(fevals + "; Total evaluations");
infoFile.close();
outFile.close();
}
qn.free();
return x;
}
